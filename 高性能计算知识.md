
# 1. 高性能计算简介

>>高性能计算(High performance computing， 缩写 HPC) 指通常使用很多处理器（作为单
个机器的一部分）或者某一集群中组织的几台计算机（作为单个计 算资源操作）的计算系
统和环境。有许多类型的 HPC 系统，其范围从标准计算机的大型集群，到高度专用的硬件。
大多数基于集群的 HPC 系统使用高性能网络互连，比如那些来自 InfiniBand 或 Myrinet 的
网络互连。基本的网络拓扑和组织可以使用一个简单的总线拓扑，在性能很高的环境中，网
状网络系统在主机之间提供较短的潜伏期，所以可改善总体网络性能和传输速率。

## 1.1. 高性能计算硬件结构

>>高性能计算拓扑结构如图 1 所示，从硬件结构上，高性能计算系统包含`计算节点`、 `IO
节点`、`登录节点`、`管理节点`、`高速网络`、`存储系统`等组成。 

![](https://github.com/cheonn/HPC-knowledge/blob/master/img/hpc.png)

                        图1  hpc硬件架构

# 1.2. 计算节点

>>计算节点是高性能集群中的最主要的计算能力的体现， 目前， 主流的计算节点有同构节
点和异构节点两种类型。

## 2.1. 同构计算节点

>>同构计算节点是指集群中每个计算节点完全有 CPU 计算资源组成， 目前， 在一个计算
节点上可以支持单路、 双路、 四路、 八路等 CPU 计算节点。
>>Intel 和 AMD CPU 型号、参数详见 http://www.techpowerup.com/cpudb

## 2.2. 异构计算节点

>>异构计算技术从 80 年代中期产生，由于它能经济有效地获取高性能计算能力、可扩展
性好、计算资源利用率高、发展潜力巨大，目前已成为并行/分布计算领域中的研究热点之
一。 异构计算的目的一般是加速和节能。
>>目前，主流的异构计算有： CPU+GPU， CPU+MIC， CPU+FPGA

### 2.2.1. CPU+GPU 异构计算

>>在 CPU+GPU 异构计算中，用 CPU 进行复杂逻辑和事务处理等串行计算，
用 GPU 完成大规模并行计算，即可以各尽其能，充分发挥计算系统的处理能力。
由于 CPU+GPU 异构系统上，每个节点 CPU 的核数也比较多，也具有一定的计
算能力，因此， CPU 除了做一些复杂逻辑和事务处理等串行计算，也可以与 GPU
一起做一部分并行计算，做到真正的 CPU+GPU 异构协同计算。
>>目前，主流的 GPU 厂商有 NVIDIA 和 AMD。 各 GPU 详细参数请查阅：
http://www.techpowerup.com/gpudb/

### 2.2.2. CPU+MIC 异构计算

>>2012 年底， Intel 公司正式推出了基于集成众核（ Many Integrated Core, MIC）架构的至
强融核（ Intel® Xeon Phi™）系列产品，用于解决高度并行计算问题。第一代 MIC 正式产品为
KNC（ Knights Corner），该产品双精度性能达到每秒一万亿次以上。
>>各型号 MIC 卡详细参数请查阅： http://www.techpowerup.com/gpudb/

# 3.存储系统

## 3.1. 存储网络

### 3.1.1. DAS (Direct Attached Storage)
>>直接连接存储（ Direct Attached Storage， DAS），是指将外置存储设备通过连接电缆，直
接连接到一台计算机上。

![](https://github.com/cheonn/HPC-knowledge/blob/master/img/DAS.png)

>>采用直接外挂存储方案的服务器结构如同 PC 架构，外部数据存储设备采用 SCSI 技术或
者 FC（ Fibre Channel）技术，直接挂接在内部总线上，数据存储是整个服务器结构的一部分。
DAS 这种直连方式，能够解决单台服务器的存储空间扩展和高性能传输的需求，并且单台外
置存储系统的容量，已经从不到 1TB 发展到了 2TB。
>>开放系统的直连式存储（ Direct-Attached Storage，简称 DAS）已经有近四十年的使用历
史，随着用户数据的不断增长，尤其是数百 GB 以上时，其在备份、恢复、扩展、灾备等方
面的问题变得日益困扰系统管理员。
>>DAS 的优缺点
>>直连式存储依赖服务器主机操作系统进行数据的 IO 读写和存储维护管理，数据备份和
恢复要求占用服务器主机资源（包括 CPU、系统 IO 等），数据流需要回流主机再到服务器连
接着的磁带机（库），数据备份通常占用服务器主机资源 20-30%，因此许多企业用户的日常
数据备份常常在深夜或业务系统不繁忙时进行，以免影响正常业务系统的运行。直连式存储
的数据量越大，备份和恢复的时间就越长，对服务器硬件的依赖性和影响就越大。
>>直连式存储与服务器主机之间的连接通道通常采用 SCSI 连接，带宽为 10MB/s、20MB/s、
40MB/s、 80MB/s 等，随着服务器 CPU 的处理能力越来越强，存储硬盘空间越来越大，阵列
的硬盘数量越来越多， SCSI 通道将会成为 IO 瓶颈；服务器主机 SCSI ID 资源有限，能够建立
的 SCSI 通道连接有限。
>>无论直连式存储还是服务器主机的扩展，从一台服务器扩展为多台服务器组成的群集
(Cluster)，或存储阵列容量的扩展，都会造成业务系统的停机，从而给企业带来经济损失，
对于银行、电信、传媒等行业 7×24 小时服务的关键业务系统，这是不可接受的。并且直连
式存储或服务器主机的升级扩展，只能由原设备厂商提供，往往受原设备厂商限制。

### 3.1.2. NAS (Network Attached Storage)

>>网络附加存储（ Network Attached Storage， NAS）， NAS 是一种专业的网络文件存储及文
件备份设备，它是基于局域网（ LAN）的，采用 TCP/IP 协议，通过网络交换机连接存储系统
和服务器主机，建立专用于数据存储的存储私网。以文件的输入/输出（ I/O）方式进行数据
传输。在 LAN 环境下， NAS 已经完全可以实现不同平台之间的数据共享，如 NT、 UNIX、 Mac、
Linux 等平台的共享。一个 NAS 系统包括处理器，文件服务管理模块和多个磁盘驱动器（用
于数据的存储）。采用网页浏览器就可以对 NAS 设备进行直观方便的管理。
>>实际上 NAS 是一个带有瘦服务器（ Thin Server）的存储设备，其作用类似于一个专用的
文件服务器。这种专用存储服务器不同于传统的通用服务器，它去掉了通用的服务器原有的
不适用大多数计算功能，而仅仅提供文件系统功能，用于存储服务，大大降低了存储设备的
成本。与传统的服务器相比， NAS 不仅响应速度快，而且数据传输速率也较高。
>>NAS 具有较好的协议独立性，支持 UNIX、 NetWare、 Windows、 OS/2 或 Internet Web 的
数据访问，客户端也不需要任何专用的软件，安装简易，甚至可以充当其他主机的网络驱动
器，可以方便地利用现有的管理工具进行管理。
>>NAS 可以通过交换机方便地接入到用户网络上，是一种即插即用的网络设备。

![](https://github.com/cheonn/HPC-knowledge/blob/master/img/NAS.png)

### 3.1.3. SAN (Storage Area Network)

>>存储区域网络（ Storage Area Network， SAN），是指采用光纤信道（ Fibre Channel）技术，
通过光纤信道交换机连接服务器主机和存储阵列，建立专用于数据存储的区域网络。
>>SAN 是专门连接存储外围设备和服务器的网络。它通常包括服务器、外部存储设备、服
务器适配器、集线器、交换机以及网络、存储管理工具等。 SAN 在综合了网络的灵活性、可
管理性及可扩展性的同时，提高了网络的带宽和存储 I/O的可靠性。它降低了存储管理费用，
并平衡了开放式系统服务器的存储能力和性能，为企业级存储应用提出了解决方案。 SAN 独
立于应用服务器网络系统之外，拥有几乎无限的存储能力，它采用高速的光纤信道作为传输
媒介， FC（光纤信道， Fibre Channel） +SCSI 的应用协议作为存储访问协议，将存储系统网络
化，实现了真正高速的共享存储。

![](https://github.com/cheonn/HPC-knowledge/blob/master/img/SAN.png)

### 3.1.4. 存储网络的三种形态比较

![](https://github.com/cheonn/HPC-knowledge/blob/master/img/比较.png)
